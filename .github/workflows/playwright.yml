name: ğŸ­ Playwright Tests with Advanced Reporting

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test against'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      browser:
        description: 'Browser to test'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - chromium
          - chrome
          - firefox
      test_pattern:
        description: 'Test pattern to run (e.g., "*.spec.js" or specific test file)'
        required: false
        default: ''
        type: string
      workers:
        description: 'Number of parallel workers'
        required: false
        default: '4'
        type: choice
        options:
          - '1'
          - '2'
          - '4'
          - '8'
      debug_mode:
        description: 'Enable debug mode with headed browser'
        required: false
        default: false
        type: boolean
      upload_trace:
        description: 'Upload trace files for failed tests'
        required: false
        default: true
        type: boolean
      skip_ai_analysis:
        description: 'Skip AI failure analysis to save time'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  issues: write
  pull-requests: write
  actions: read
  checks: write

env:
  NODE_VERSION: '20'
  PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: '1'

jobs:
  # Build and prepare job
  build:
    name: ğŸ—ï¸ Build & Prepare
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ¯ Display Manual Run Configuration
        if: github.event_name == 'workflow_dispatch'
        run: |
          echo "ğŸš€ Manual Test Run Configuration:"
          echo "ğŸ“ Environment: ${{ github.event.inputs.environment || 'staging' }}"
          echo "ğŸŒ Browser: ${{ github.event.inputs.browser || 'all' }}"
          echo "ğŸ§ª Test Pattern: ${{ github.event.inputs.test_pattern || 'all tests' }}"
          echo "ğŸ‘¥ Workers: ${{ github.event.inputs.workers || '4' }}"
          echo "ğŸ› Debug Mode: ${{ github.event.inputs.debug_mode || 'false' }}"
          echo "ğŸ“Š Upload Trace: ${{ github.event.inputs.upload_trace || 'true' }}"
          echo "ğŸ¤– Skip AI Analysis: ${{ github.event.inputs.skip_ai_analysis || 'false' }}"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ğŸ“¦ Install dependencies
        run: npm ci

      - name: ğŸ”¨ Build TypeScript
        run: npm run build
        continue-on-error: true

      - name: ğŸ” Generate cache key
        id: cache-key
        run: echo "key=deps-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}" >> $GITHUB_OUTPUT

      - name: ğŸ’¾ Cache node modules
        uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ steps.cache-key.outputs.key }}

  # Test job with matrix strategy
  test:
    name: ğŸ§ª Test (${{ matrix.browser }})
    runs-on: ubuntu-latest
    needs: build
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, chrome, firefox]
    
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4
        
      - name: ğŸ” Check if browser should run
        id: should-run
        run: |
          BROWSER="${{ matrix.browser }}"
          INPUT_BROWSER="${{ github.event.inputs.browser || 'all' }}"
          
          if [[ "$INPUT_BROWSER" == "all" || "$INPUT_BROWSER" == "$BROWSER" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi

      - name: ğŸ”§ Setup Node.js
        if: steps.should-run.outputs.should_run == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ğŸ“¦ Restore dependencies
        if: steps.should-run.outputs.should_run == 'true'
        uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ needs.build.outputs.cache-key }}

      - name: ğŸ“¦ Install dependencies (fallback)
        if: steps.should-run.outputs.should_run == 'true' && steps.cache.outputs.cache-hit != 'true'
        run: npm ci

      - name: ğŸ­ Install Playwright Browsers
        if: steps.should-run.outputs.should_run == 'true'
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: ğŸ“ Prepare test directories
        if: steps.should-run.outputs.should_run == 'true'
        run: |
          mkdir -p test-results
          mkdir -p playwright-report

      - name: ğŸ§ª Run Playwright tests
        if: steps.should-run.outputs.should_run == 'true'
        run: |
          # Build the test command with manual inputs
          CMD="npx playwright test --project=${{ matrix.browser }}"
          
          # Add workers if specified
          if [ "${{ github.event.inputs.workers }}" != "" ]; then
            CMD="$CMD --workers=${{ github.event.inputs.workers }}"
          fi
          
          # Add test pattern if specified
          if [ "${{ github.event.inputs.test_pattern }}" != "" ]; then
            CMD="$CMD ${{ github.event.inputs.test_pattern }}"
          fi
          
          # Add debug mode if enabled
          if [ "${{ github.event.inputs.debug_mode }}" == "true" ]; then
            CMD="$CMD --headed --debug"
          fi
          
          # Add trace options based on input
          if [ "${{ github.event.inputs.upload_trace }}" == "true" ]; then
            CMD="$CMD --trace=on-first-retry"
          else
            CMD="$CMD --trace=off"
          fi
          
          echo "ğŸ¯ Executing: $CMD"
          eval $CMD
        env:
          TEST_ENV: ${{ github.event.inputs.environment || 'staging' }}
          CI: true
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: ğŸ“Š Generate CTRF Summary
        if: always() && steps.should-run.outputs.should_run == 'true'
        run: |
          if [ -f "test-results/ctrf-report.json" ]; then
            npm run ctrf:summary
          else
            echo "No CTRF report found in test-results/"
          fi

      - name: ğŸ¤– Generate AI Analysis
        if: always() && steps.should-run.outputs.should_run == 'true' && github.event.inputs.skip_ai_analysis != 'true'
        run: |
          if [ -f "test-results/ctrf-report.json" ]; then
            echo "Generating AI analysis for test results..."
            npm run ai:report
          else
            echo "No CTRF report found, skipping AI analysis"
          fi
        continue-on-error: true
        env:
          AI_PROVIDER: ${{ secrets.AI_PROVIDER || 'ollama' }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}
          BEDROCK_MODEL: ${{ secrets.BEDROCK_MODEL || 'anthropic.claude-3-sonnet-20240229-v1:0' }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CLAUDE_MODEL: ${{ secrets.CLAUDE_MODEL || 'claude-3-5-sonnet-20240620' }}
          OPENAI_MODEL: ${{ secrets.OPENAI_MODEL || 'gpt-4o' }}
          OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL || 'llama3.2' }}

      - name: ğŸ§  Enhanced AI-CTRF Analysis
        if: always() && steps.should-run.outputs.should_run == 'true' && github.event.inputs.skip_ai_analysis != 'true'
        run: |
          if [ -f "ctrf/test-results/ctrf-report.json" ]; then
            echo "ğŸš€ Running enhanced AI-CTRF analysis..."
            npm run ai:smart
          else
            echo "âš ï¸  No CTRF report found for AI-CTRF analysis"
          fi
        continue-on-error: true
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: ğŸ“Š Display AI Insights Summary  
        if: always() && steps.should-run.outputs.should_run == 'true'
        run: |
          echo "=================================================================================="
          echo "ğŸ¤– AI TEST ANALYSIS SUMMARY (${{ matrix.browser }})"
          echo "=================================================================================="
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract test summary
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0') 
            TOTAL=$(cat test-results/ctrf-report.json | jq -r '.results.summary.tests // 0')
            
            echo "ğŸ“Š Test Results: $PASSED passed, $FAILED failed, $TOTAL total"
            echo ""
            
            if [ "$FAILED" -gt 0 ]; then
              echo "âŒ Failed Test Analysis:"
              echo "----------------------------------------"
              
              # Show AI analysis for failed tests (first 2)
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                select(.ai != null) |
                "ğŸ” " + .name + ":\n" + (.ai | split("**")[1:] | join("") | split("\n")[0:3] | join(" ") | .[0:200]) + "...\n"
              ' | head -20
              
              if [ -f "ai-test-report-*.html" ]; then
                echo "ğŸ“„ Full AI report available in workflow artifacts"
              fi
            else
              echo "âœ… All tests passed! No AI analysis needed."
            fi
          else
            echo "âš ï¸  No test results available for AI analysis"
          fi
          
          echo "=================================================================================="

      - name: ğŸ¯ Display Browser-Specific AI Insights
        if: always() && steps.should-run.outputs.should_run == 'true'
        run: |
          echo "=========================================="
          echo "ğŸ­ ${{ matrix.browser }} AI ANALYSIS"
          echo "=========================================="
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract test summary
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0')
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            
            echo "ğŸ“Š Browser: ${{ matrix.browser }}"
            echo "   âœ… Passed: $PASSED"
            echo "   âŒ Failed: $FAILED"
            echo ""
            
            if [ "$FAILED" -gt 0 ]; then
              echo "ğŸ” FAILED TESTS (AI INSIGHTS):"
              echo "----------------------------------------"
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                "âŒ " + .name + "\n" +
                if .ai then "   ğŸ’¡ " + (.ai | split("\n")[0]) + "\n" else "   ğŸ’¡ AI analysis not available\n" end +
                "   â±ï¸  " + (.duration | tostring) + "ms\n"
              '
            else
              echo "ğŸ‰ All ${{ matrix.browser }} tests passed!"
            fi
          else
            echo "âŒ No test results available for ${{ matrix.browser }}"
          fi
          echo "=========================================="

      - name: ğŸ“ˆ Upload CTRF Report
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ctrf-report-${{ matrix.browser }}
          path: |
            test-results/ctrf-report.json
            ctrf/
          retention-days: 30

      - name: ğŸ¤– Upload AI Analysis Report
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ai-report-${{ matrix.browser }}
          path: |
            ai-test-report-*.html
            ai-reports/
          retention-days: 30

      - name: ğŸ§  Upload AI-CTRF Enhanced Reports
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ai-ctrf-reports-${{ matrix.browser }}
          path: |
            ai-reports/
            ctrf/test-results/ctrf-report.json
          retention-days: 30

      - name: ğŸ“‹ Upload Playwright Report
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report-${{ matrix.browser }}
          path: playwright-report/
          retention-days: 30

      - name: ğŸ“¸ Upload Screenshots
        if: failure() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: screenshots-${{ matrix.browser }}
          path: |
            test-results/
            *.png
          retention-days: 7

  # AI Analysis job (consolidates reports from all browsers)
  ai-analysis:
    name: ğŸ¤– Consolidated AI Test Analysis
    runs-on: ubuntu-latest
    needs: test
    if: always() && github.event.inputs.skip_ai_analysis != 'true'
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ğŸ“¦ Install dependencies
        run: npm ci

      - name: ï¿½ Build TypeScript
        run: npm run build

      - name: ï¿½ğŸ“¥ Download CTRF reports from all browsers
        uses: actions/download-artifact@v4
        with:
          pattern: ctrf-report-*
          path: downloaded-reports/
          merge-multiple: true

      - name: ğŸ”„ Consolidate CTRF Reports
        run: |
          echo "Consolidating CTRF reports from all browsers..."
          mkdir -p consolidated-reports
          mkdir -p test-results
          
          # Debug: Show directory structure
          echo "Directory structure after artifact download:"
          find downloaded-reports -type f -name "*.json" || echo "No JSON files found"
          echo ""
          
          # Find all CTRF reports
          echo "Looking for CTRF reports..."
          CTRF_FILES=($(find downloaded-reports -name "ctrf-report.json" -type f))
          
          if [ ${#CTRF_FILES[@]} -eq 0 ]; then
            echo "âŒ No CTRF reports found in downloaded artifacts"
            echo "Creating empty report structure..."
            echo '{"results":{"tool":{"name":"Playwright"},"summary":{"tests":0,"passed":0,"failed":0,"skipped":0,"pending":0,"other":0,"start":0,"stop":0},"tests":[]}}' > test-results/ctrf-report.json
          else
            echo "âœ… Found ${#CTRF_FILES[@]} CTRF reports:"
            
            # Copy each report with counter-based naming
            counter=1
            for file in "${CTRF_FILES[@]}"; do
              echo "  Processing: $file"
              cp "$file" "consolidated-reports/ctrf-report-browser${counter}.json"
              echo "  â†’ Copied to: consolidated-reports/ctrf-report-browser${counter}.json"
              counter=$((counter + 1))
            done
            
            echo ""
            echo "Consolidated reports created:"
            ls -la consolidated-reports/
            
            # Use the first report as the primary report
            FIRST_REPORT=$(ls consolidated-reports/ctrf-report*.json | head -1)
            if [ -f "$FIRST_REPORT" ]; then
              cp "$FIRST_REPORT" test-results/ctrf-report.json
              echo "âœ… Primary report set: $FIRST_REPORT â†’ test-results/ctrf-report.json"
            else
              echo "âŒ Failed to find first report, creating empty structure"
              echo '{"results":{"tool":{"name":"Playwright"},"summary":{"tests":0,"passed":0,"failed":0,"skipped":0,"pending":0,"other":0,"start":0,"stop":0},"tests":[]}}' > test-results/ctrf-report.json
            fi
          fi
          
          # Verify final structure
          echo ""
          echo "Final test-results directory:"
          ls -la test-results/

      - name: ğŸ¤– Generate Comprehensive AI Analysis
        run: |
          if [ -f "test-results/ctrf-report.json" ]; then
            echo "Generating comprehensive AI analysis for all test results..."
            npm run ai:report
            
            # Display summary of what was generated
            echo "AI Analysis Generated:"
            ls -la ai-test-report-*.html 2>/dev/null || echo "No HTML reports found"
            ls -la test-results/ | grep -E "(ctrf|json)" || echo "No CTRF files found"
          else
            echo "No consolidated CTRF report found, skipping AI analysis"
          fi
        continue-on-error: true
        env:
          AI_PROVIDER: ${{ secrets.AI_PROVIDER || 'bedrock' }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}
          BEDROCK_MODEL: ${{ secrets.BEDROCK_MODEL || 'anthropic.claude-3-sonnet-20240229-v1:0' }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CLAUDE_MODEL: ${{ secrets.CLAUDE_MODEL || 'claude-3-5-sonnet-20240620' }}
          OPENAI_MODEL: ${{ secrets.OPENAI_MODEL || 'gpt-4o' }}
          OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL || 'llama3.2' }}

      - name: ğŸ¯ Display Comprehensive AI Analysis
        run: |
          echo "=================================================================================="
          echo "ğŸ¤– COMPREHENSIVE AI TEST ANALYSIS - ALL BROWSERS"
          echo "=================================================================================="
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract comprehensive test summary
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0')
            SKIPPED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.skipped // 0') 
            TOTAL=$(cat test-results/ctrf-report.json | jq -r '.results.summary.tests // 0')
            DURATION=$(cat test-results/ctrf-report.json | jq -r '(.results.summary.stop - .results.summary.start) / 1000 | floor')
            
            echo "ğŸ“Š Final Test Results:"
            echo "   âœ… Passed: $PASSED"
            echo "   âŒ Failed: $FAILED" 
            echo "   â­ï¸ Skipped: $SKIPPED"
            echo "   ğŸ“Š Total: $TOTAL"
            echo "   â±ï¸ Duration: ${DURATION}s"
            echo ""
            
            if [ "$FAILED" -gt 0 ]; then
              echo "ğŸ” AI Analysis of Failed Tests:"
              echo "----------------------------------------"
              
              # Show detailed AI analysis summary
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                select(.ai != null) |
                "âŒ " + .name + " (" + (.suite // "unknown") + ")" + "\n" +
                "   Duration: " + (.duration | tostring) + "ms\n" + 
                "   AI Analysis: " + (.ai | 
                  split("**Root Cause:**")[1]? // 
                  split("**Technical Details:**")[0]? //
                  split("\n")[0:2] | join(" ") | .[0:150]
                ) + "...\n"
              '
              
              echo ""
              echo "ğŸ“Š Overall AI Summary:"
              if [ -f "test-results/ctrf-report.json" ]; then
                OVERALL_AI=$(cat test-results/ctrf-report.json | jq -r '.results.extra.ai // empty')
                if [ ! -z "$OVERALL_AI" ]; then
                  echo "$OVERALL_AI" | head -5
                else
                  echo "   ğŸ”„ Individual test analyses available above"
                fi
              fi
              
              echo ""
              echo "ğŸ“„ Complete analysis available in 'comprehensive-ai-analysis' artifact"
              echo "ğŸŒ Download the HTML report for full interactive analysis"
              
            else
              echo "ğŸ‰ All tests passed across all browsers!"
              echo "âœ¨ No issues detected - great job!"
            fi
          else
            echo "âš ï¸  No consolidated test results available"
          fi
          
          echo "=================================================================================="

      - name: ğŸ“‹ Display AI Analysis Summary in Workflow
        if: always()
        run: |
          echo "============================================================"
          echo "ğŸ¤– AI TEST ANALYSIS SUMMARY"
          echo "============================================================"
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract test summary from CTRF report
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0') 
            SKIPPED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.skipped // 0')
            TOTAL=$(cat test-results/ctrf-report.json | jq -r '.results.summary.tests // 0')
            DURATION=$(cat test-results/ctrf-report.json | jq -r '(.results.summary.stop - .results.summary.start) / 1000 | floor')
            
            echo "ğŸ“Š TEST RESULTS:"
            echo "   âœ… Passed:  $PASSED"
            echo "   âŒ Failed:  $FAILED" 
            echo "   â­ï¸ Skipped: $SKIPPED"
            echo "   ğŸ“Š Total:   $TOTAL"
            echo "   â±ï¸ Duration: ${DURATION}s"
            echo ""
            
            # Display failed tests with AI analysis
            if [ "$FAILED" -gt 0 ]; then
              echo "ğŸ” FAILED TESTS WITH AI INSIGHTS:"
              echo "============================================================"
              
              # Extract failed tests and their AI analysis from CTRF
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                "ğŸš¨ Test: " + .name + "\n" +
                "   Suite: " + (.suite // "N/A") + "\n" +
                "   Duration: " + (.duration | tostring) + "ms\n" +
                if .ai then "   ğŸ¤– AI Analysis: " + (.ai | gsub("\n"; "\n      ")) + "\n" else "   ğŸ¤– AI Analysis: Not available\n" end +
                "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
              '
            else
              echo "ğŸ‰ All tests passed! No failures to analyze."
            fi
            
            # Display overall AI summary if available
            OVERALL_AI=$(cat test-results/ctrf-report.json | jq -r '.results.extra.ai // empty')
            if [ ! -z "$OVERALL_AI" ]; then
              echo ""
              echo "ğŸ“‹ OVERALL AI SUMMARY:"
              echo "============================================================"
              echo "$OVERALL_AI"
            fi
            
          else
            echo "âŒ No test results found to display"
          fi
          
          echo ""
          echo "============================================================"
          echo "ğŸ“ Full AI report available in artifacts: 'comprehensive-ai-analysis'"
          echo "============================================================"

      - name: ğŸ“‹ Upload Comprehensive AI Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-ai-analysis
          path: |
            ai-test-report-*.html
            test-results/ctrf-report.json
            consolidated-reports/
          retention-days: 30

      - name: ğŸ“Š Generate Test Summary for PR
        if: always()
        run: |
          echo "Generating test summary for GitHub PR comment..."
          if [ -f "test-results/ctrf-report.json" ]; then
            echo "TEST_SUMMARY<<EOF" >> $GITHUB_ENV
            cat test-results/ctrf-report.json | jq -r '
              .results.summary as $summary |
              .results.tests as $tests |
              "## ğŸ­ Playwright Test Results\n\n" +
              "| Metric | Value |\n" +
              "|--------|---------|\n" +
              "| âœ… Passed | \($summary.passed) |\n" +
              "| âŒ Failed | \($summary.failed) |\n" +
              "| â­ï¸ Skipped | \($summary.skipped) |\n" +
              "| ğŸ“Š Total | \($summary.tests) |\n" +
              "| â±ï¸ Duration | \(($summary.stop - $summary.start) / 1000 | floor)s |\n\n" +
              "**Environment:** " + (env.TEST_ENV // "staging") + "\n\n" +
              (if $summary.failed > 0 then 
                "### ğŸ¤– AI Analysis Summary\n\n" +
                ([$tests[] | select(.status == "failed")] | map("- **" + .name + "**: " + (if .ai then (.ai | split("\n")[0] | .[0:100] + (if length > 100 then "..." else "" end)) else "Analysis pending" end)) | join("\n")) + "\n\n"
              else 
                "### ğŸ‰ All Tests Passed!\nNo failures to analyze.\n\n" 
              end) +
              "ğŸ“ **Full AI Report:** Download `comprehensive-ai-analysis` artifact for detailed insights"
            ' >> $GITHUB_ENV
            echo "EOF" >> $GITHUB_ENV
          else
            echo "TEST_SUMMARY=No test results available" >> $GITHUB_ENV
          fi

  # Report consolidation job
  consolidate-reports:
    name: ğŸ“Š Consolidate Reports
    runs-on: ubuntu-latest
    needs: [test, ai-analysis]
    if: always()
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ï¿½ Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*-report-*"
          path: all-artifacts/
          merge-multiple: true

      - name: ï¿½ Download AI analysis
        uses: actions/download-artifact@v4
        with:
          name: comprehensive-ai-analysis
          path: ai-analysis/

      - name: ï¿½ Create Final Report Package
        run: |
          mkdir -p final-reports
          
          # Copy CTRF reports
          find all-artifacts -name "ctrf-report.json" -exec cp {} final-reports/ \; 2>/dev/null || true
          
          # Copy Playwright reports
          find all-artifacts -name "index.html" -path "*/playwright-report/*" -exec cp {} final-reports/playwright-report.html \; 2>/dev/null || true
          
          # Copy AI analysis
          cp ai-analysis/ai-test-report-*.html final-reports/ 2>/dev/null || true
          cp ai-analysis/test-results/ctrf-report.json final-reports/consolidated-ctrf-report.json 2>/dev/null || true
          
          # Generate a summary index
          cat > final-reports/README.md << 'EOF'
          # Test Report Summary
          
          This package contains comprehensive test results and AI analysis.
          
          ## Files Included:
          - `ai-test-report-*.html` - AI-powered analysis of test failures
          - `playwright-report.html` - Standard Playwright HTML report  
          - `ctrf-report.json` - Machine-readable test results
          - `consolidated-ctrf-report.json` - Final consolidated results
          
          ## How to Use:
          1. Open the AI test report HTML file in a browser for intelligent insights
          2. Review the Playwright report for detailed test execution logs
          3. Use the CTRF JSON for programmatic analysis
          
          Generated by: Playwright Tests with Advanced Reporting
          EOF
          
          echo "Final report package contents:"
          ls -la final-reports/

      - name: ï¿½ Upload Final Report Package  
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: complete-test-report-package
          path: final-reports/
          retention-days: 90

      - name: ğŸ“ˆ Comment PR with Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const testSummary = process.env.TEST_SUMMARY || "No test results available";
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: testSummary + "\n\n" +
                    "---\n" +
                    "ğŸ¤– **AI-Powered Analysis**: Intelligent test failure analysis included\n" +
                    "ğŸ“‹ **Workflow Logs**: Real-time AI insights visible in workflow output\n" +
                    "ğŸ“¦ **Download Reports**: Complete analysis available in workflow artifacts\n" +
                    "ï¿½ **Quick Access**: Check the 'Display AI Analysis' steps for immediate insights"
            });
