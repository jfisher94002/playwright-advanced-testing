name: üé≠ Playwright Tests with Advanced Reporting

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test against'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      browser:
        description: 'Browser to test'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - chromium
          - chrome
          - firefox
      test_pattern:
        description: 'Test pattern to run (e.g., "*.spec.js" or specific test file)'
        required: false
        default: ''
        type: string
      workers:
        description: 'Number of parallel workers'
        required: false
        default: '4'
        type: choice
        options:
          - '1'
          - '2'
          - '4'
          - '8'
      debug_mode:
        description: 'Enable debug mode with headed browser'
        required: false
        default: false
        type: boolean
      upload_trace:
        description: 'Upload trace files for failed tests'
        required: false
        default: true
        type: boolean
      skip_ai_analysis:
        description: 'Skip AI failure analysis to save time'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  issues: write
  pull-requests: write
  actions: read
  checks: write

env:
  NODE_VERSION: '20'
  PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: '1'

jobs:
  # Build and prepare job
  build:
    name: üèóÔ∏è Build & Prepare
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üéØ Display Manual Run Configuration
        if: github.event_name == 'workflow_dispatch'
        run: |
          echo "üöÄ Manual Test Run Configuration:"
          echo "üìç Environment: ${{ github.event.inputs.environment || 'staging' }}"
          echo "üåê Browser: ${{ github.event.inputs.browser || 'all' }}"
          echo "üß™ Test Pattern: ${{ github.event.inputs.test_pattern || 'all tests' }}"
          echo "üë• Workers: ${{ github.event.inputs.workers || '4' }}"
          echo "üêõ Debug Mode: ${{ github.event.inputs.debug_mode || 'false' }}"
          echo "üìä Upload Trace: ${{ github.event.inputs.upload_trace || 'true' }}"
          echo "ü§ñ Skip AI Analysis: ${{ github.event.inputs.skip_ai_analysis || 'false' }}"
          echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"

      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: üì¶ Install dependencies
        run: npm ci

      - name: üî® Build TypeScript
        run: npm run build
        continue-on-error: true

      - name: üîç Generate cache key
        id: cache-key
        run: echo "key=deps-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}" >> $GITHUB_OUTPUT

      - name: üíæ Cache node modules
        uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ steps.cache-key.outputs.key }}

  # Test job with matrix strategy
  test:
    name: üß™ Test (${{ matrix.browser }})
    runs-on: ubuntu-latest
    needs: build
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, chrome, firefox]
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        
      - name: üîç Check if browser should run
        id: should-run
        run: |
          BROWSER="${{ matrix.browser }}"
          INPUT_BROWSER="${{ github.event.inputs.browser || 'all' }}"
          
          if [[ "$INPUT_BROWSER" == "all" || "$INPUT_BROWSER" == "$BROWSER" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi

      - name: üîß Setup Node.js
        if: steps.should-run.outputs.should_run == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: üì¶ Restore dependencies
        if: steps.should-run.outputs.should_run == 'true'
        uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ needs.build.outputs.cache-key }}

      - name: üì¶ Install dependencies (fallback)
        if: steps.should-run.outputs.should_run == 'true' && steps.cache.outputs.cache-hit != 'true'
        run: npm ci

      - name: üé≠ Install Playwright Browsers
        if: steps.should-run.outputs.should_run == 'true'
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: üìÅ Prepare test directories
        if: steps.should-run.outputs.should_run == 'true'
        run: |
          mkdir -p test-results
          mkdir -p playwright-report

      - name: üß™ Run Playwright tests
        if: steps.should-run.outputs.should_run == 'true'
        run: |
          # Build the test command with manual inputs
          CMD="npx playwright test --project=${{ matrix.browser }}"
          
          # Add workers if specified
          if [ "${{ github.event.inputs.workers }}" != "" ]; then
            CMD="$CMD --workers=${{ github.event.inputs.workers }}"
          fi
          
          # Add test pattern if specified
          if [ "${{ github.event.inputs.test_pattern }}" != "" ]; then
            CMD="$CMD ${{ github.event.inputs.test_pattern }}"
          fi
          
          # Add debug mode if enabled
          if [ "${{ github.event.inputs.debug_mode }}" == "true" ]; then
            CMD="$CMD --headed --debug"
          fi
          
          # Add trace options based on input
          if [ "${{ github.event.inputs.upload_trace }}" == "true" ]; then
            CMD="$CMD --trace=on-first-retry"
          else
            CMD="$CMD --trace=off"
          fi
          
          echo "üéØ Executing: $CMD"
          eval $CMD
        env:
          TEST_ENV: ${{ github.event.inputs.environment || 'staging' }}
          CI: true
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: üìä Generate CTRF Summary
        if: always() && steps.should-run.outputs.should_run == 'true'
        run: |
          if [ -f "test-results/ctrf-report.json" ]; then
            npm run ctrf:summary
          else
            echo "No CTRF report found in test-results/"
          fi

      - name: ü§ñ Generate AI Analysis
        if: always() && steps.should-run.outputs.should_run == 'true' && github.event.inputs.skip_ai_analysis != 'true'
        run: |
          if [ -f "test-results/ctrf-report.json" ]; then
            echo "Generating AI analysis for test results..."
            npm run ai:report
          else
            echo "No CTRF report found, skipping AI analysis"
          fi
        continue-on-error: true
        env:
          AI_PROVIDER: ${{ secrets.AI_PROVIDER || 'ollama' }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}
          BEDROCK_MODEL: ${{ secrets.BEDROCK_MODEL || 'anthropic.claude-3-sonnet-20240229-v1:0' }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CLAUDE_MODEL: ${{ secrets.CLAUDE_MODEL || 'claude-3-5-sonnet-20240620' }}
          OPENAI_MODEL: ${{ secrets.OPENAI_MODEL || 'gpt-4o' }}
          OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL || 'llama3.2' }}

      - name: üß† Enhanced AI-CTRF Analysis
        if: always() && steps.should-run.outputs.should_run == 'true' && github.event.inputs.skip_ai_analysis != 'true'
        run: |
          if [ -f "ctrf/test-results/ctrf-report.json" ]; then
            echo "üöÄ Running enhanced AI-CTRF analysis..."
            npm run ai:smart
          else
            echo "‚ö†Ô∏è  No CTRF report found for AI-CTRF analysis"
          fi
        continue-on-error: true
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          GROK_API_KEY: ${{ secrets.GROK_API_KEY }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: üìä Display AI Insights Summary  
        if: always() && steps.should-run.outputs.should_run == 'true'
        run: |
          echo "=================================================================================="
          echo "ü§ñ AI TEST ANALYSIS SUMMARY (${{ matrix.browser }})"
          echo "=================================================================================="
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract test summary
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0') 
            TOTAL=$(cat test-results/ctrf-report.json | jq -r '.results.summary.tests // 0')
            
            echo "üìä Test Results: $PASSED passed, $FAILED failed, $TOTAL total"
            echo ""
            
            if [ "$FAILED" -gt 0 ]; then
              echo "‚ùå Failed Test Analysis:"
              echo "----------------------------------------"
              
              # Show AI analysis for failed tests (first 2)
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                select(.ai != null) |
                "üîç " + .name + ":\n" + (.ai | split("**")[1:] | join("") | split("\n")[0:3] | join(" ") | .[0:200]) + "...\n"
              ' | head -20
              
              if [ -f "ai-test-report-*.html" ]; then
                echo "üìÑ Full AI report available in workflow artifacts"
              fi
            else
              echo "‚úÖ All tests passed! No AI analysis needed."
            fi
          else
            echo "‚ö†Ô∏è  No test results available for AI analysis"
          fi
          
          echo "=================================================================================="

      - name: üéØ Display Browser-Specific AI Insights
        if: always() && steps.should-run.outputs.should_run == 'true'
        run: |
          echo "=========================================="
          echo "üé≠ ${{ matrix.browser }} AI ANALYSIS"
          echo "=========================================="
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract test summary
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0')
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            
            echo "üìä Browser: ${{ matrix.browser }}"
            echo "   ‚úÖ Passed: $PASSED"
            echo "   ‚ùå Failed: $FAILED"
            echo ""
            
            if [ "$FAILED" -gt 0 ]; then
              echo "üîç FAILED TESTS (AI INSIGHTS):"
              echo "----------------------------------------"
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                "‚ùå " + .name + "\n" +
                if .ai then "   üí° " + (.ai | split("\n")[0]) + "\n" else "   üí° AI analysis not available\n" end +
                "   ‚è±Ô∏è  " + (.duration | tostring) + "ms\n"
              '
            else
              echo "üéâ All ${{ matrix.browser }} tests passed!"
            fi
          else
            echo "‚ùå No test results available for ${{ matrix.browser }}"
          fi
          echo "=========================================="

      - name: üìà Upload CTRF Report
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ctrf-report-${{ matrix.browser }}
          path: |
            test-results/ctrf-report.json
            ctrf/
          retention-days: 30

      - name: ü§ñ Upload AI Analysis Report
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ai-report-${{ matrix.browser }}
          path: |
            ai-test-report-*.html
            ai-reports/
          retention-days: 30

      - name: üß† Upload AI-CTRF Enhanced Reports
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ai-ctrf-reports-${{ matrix.browser }}
          path: |
            ai-reports/
            ctrf/test-results/ctrf-report.json
          retention-days: 30

      - name: üìã Upload Playwright Report
        if: always() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report-${{ matrix.browser }}
          path: playwright-report/
          retention-days: 30

      - name: üì∏ Upload Screenshots
        if: failure() && steps.should-run.outputs.should_run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: screenshots-${{ matrix.browser }}
          path: |
            test-results/
            *.png
          retention-days: 7

  # AI Analysis job (consolidates reports from all browsers)
  ai-analysis:
    name: ü§ñ Consolidated AI Test Analysis
    runs-on: ubuntu-latest
    needs: test
    if: always() && github.event.inputs.skip_ai_analysis != 'true'
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: üì¶ Install dependencies
        run: npm ci

      - name: ÔøΩ Build TypeScript
        run: npm run build

      - name: ÔøΩüì• Download CTRF reports from all browsers
        uses: actions/download-artifact@v4
        with:
          pattern: ctrf-report-*
          path: downloaded-reports/
          merge-multiple: true

      - name: üîÑ Consolidate CTRF Reports
        run: |
          echo "Consolidating CTRF reports from all browsers..."
          mkdir -p consolidated-reports
          mkdir -p test-results
          
          # Debug: Show directory structure
          echo "Directory structure after artifact download:"
          find downloaded-reports -type f -name "*.json" || echo "No JSON files found"
          echo ""
          
          # Find all CTRF reports
          echo "Looking for CTRF reports..."
          CTRF_FILES=($(find downloaded-reports -name "ctrf-report.json" -type f))
          
          if [ ${#CTRF_FILES[@]} -eq 0 ]; then
            echo "‚ùå No CTRF reports found in downloaded artifacts"
            echo "Creating empty report structure..."
            echo '{"results":{"tool":{"name":"Playwright"},"summary":{"tests":0,"passed":0,"failed":0,"skipped":0,"pending":0,"other":0,"start":0,"stop":0},"tests":[]}}' > test-results/ctrf-report.json
          else
            echo "‚úÖ Found ${#CTRF_FILES[@]} CTRF reports:"
            
            # Copy each report with counter-based naming
            counter=1
            for file in "${CTRF_FILES[@]}"; do
              echo "  Processing: $file"
              cp "$file" "consolidated-reports/ctrf-report-browser${counter}.json"
              echo "  ‚Üí Copied to: consolidated-reports/ctrf-report-browser${counter}.json"
              counter=$((counter + 1))
            done
            
            echo ""
            echo "Consolidated reports created:"
            ls -la consolidated-reports/
            
            # Use the first report as the primary report
            FIRST_REPORT=$(ls consolidated-reports/ctrf-report*.json | head -1)
            if [ -f "$FIRST_REPORT" ]; then
              cp "$FIRST_REPORT" test-results/ctrf-report.json
              echo "‚úÖ Primary report set: $FIRST_REPORT ‚Üí test-results/ctrf-report.json"
            else
              echo "‚ùå Failed to find first report, creating empty structure"
              echo '{"results":{"tool":{"name":"Playwright"},"summary":{"tests":0,"passed":0,"failed":0,"skipped":0,"pending":0,"other":0,"start":0,"stop":0},"tests":[]}}' > test-results/ctrf-report.json
            fi
          fi
          
          # Verify final structure
          echo ""
          echo "Final test-results directory:"
          ls -la test-results/

      - name: ü§ñ Generate Comprehensive AI Analysis
        run: |
          if [ -f "test-results/ctrf-report.json" ]; then
            echo "Generating comprehensive AI analysis for all test results..."
            npm run ai:report
            
            # Display summary of what was generated
            echo "AI Analysis Generated:"
            ls -la ai-test-report-*.html 2>/dev/null || echo "No HTML reports found"
            ls -la test-results/ | grep -E "(ctrf|json)" || echo "No CTRF files found"
          else
            echo "No consolidated CTRF report found, skipping AI analysis"
          fi
        continue-on-error: true
        env:
          AI_PROVIDER: ${{ secrets.AI_PROVIDER || 'bedrock' }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-west-2' }}
          BEDROCK_MODEL: ${{ secrets.BEDROCK_MODEL || 'anthropic.claude-3-sonnet-20240229-v1:0' }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CLAUDE_MODEL: ${{ secrets.CLAUDE_MODEL || 'claude-3-5-sonnet-20240620' }}
          OPENAI_MODEL: ${{ secrets.OPENAI_MODEL || 'gpt-4o' }}
          OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL || 'llama3.2' }}

      - name: üéØ Display Comprehensive AI Analysis
        run: |
          echo "=================================================================================="
          echo "ü§ñ COMPREHENSIVE AI TEST ANALYSIS - ALL BROWSERS"
          echo "=================================================================================="
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract comprehensive test summary
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0')
            SKIPPED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.skipped // 0') 
            TOTAL=$(cat test-results/ctrf-report.json | jq -r '.results.summary.tests // 0')
            DURATION=$(cat test-results/ctrf-report.json | jq -r '(.results.summary.stop - .results.summary.start) / 1000 | floor')
            
            echo "üìä Final Test Results:"
            echo "   ‚úÖ Passed: $PASSED"
            echo "   ‚ùå Failed: $FAILED" 
            echo "   ‚è≠Ô∏è Skipped: $SKIPPED"
            echo "   üìä Total: $TOTAL"
            echo "   ‚è±Ô∏è Duration: ${DURATION}s"
            echo ""
            
            if [ "$FAILED" -gt 0 ]; then
              echo "üîç AI Analysis of Failed Tests:"
              echo "----------------------------------------"
              
              # Show detailed AI analysis summary
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                select(.ai != null) |
                "‚ùå " + .name + " (" + (.suite // "unknown") + ")" + "\n" +
                "   Duration: " + (.duration | tostring) + "ms\n" + 
                "   AI Analysis: " + (.ai | 
                  split("**Root Cause:**")[1]? // 
                  split("**Technical Details:**")[0]? //
                  split("\n")[0:2] | join(" ") | .[0:150]
                ) + "...\n"
              '
              
              echo ""
              echo "üìä Overall AI Summary:"
              if [ -f "test-results/ctrf-report.json" ]; then
                OVERALL_AI=$(cat test-results/ctrf-report.json | jq -r '.results.extra.ai // empty')
                if [ ! -z "$OVERALL_AI" ]; then
                  echo "$OVERALL_AI" | head -5
                else
                  echo "   üîÑ Individual test analyses available above"
                fi
              fi
              
              echo ""
              echo "üìÑ Complete analysis available in 'comprehensive-ai-analysis' artifact"
              echo "üåê Download the HTML report for full interactive analysis"
              
            else
              echo "üéâ All tests passed across all browsers!"
              echo "‚ú® No issues detected - great job!"
            fi
          else
            echo "‚ö†Ô∏è  No consolidated test results available"
          fi
          
          echo "=================================================================================="

      - name: üìã Display AI Analysis Summary in Workflow
        if: always()
        run: |
          echo "============================================================"
          echo "ü§ñ AI TEST ANALYSIS SUMMARY"
          echo "============================================================"
          
          if [ -f "test-results/ctrf-report.json" ]; then
            # Extract test summary from CTRF report
            PASSED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.passed // 0')
            FAILED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.failed // 0') 
            SKIPPED=$(cat test-results/ctrf-report.json | jq -r '.results.summary.skipped // 0')
            TOTAL=$(cat test-results/ctrf-report.json | jq -r '.results.summary.tests // 0')
            DURATION=$(cat test-results/ctrf-report.json | jq -r '(.results.summary.stop - .results.summary.start) / 1000 | floor')
            
            echo "üìä TEST RESULTS:"
            echo "   ‚úÖ Passed:  $PASSED"
            echo "   ‚ùå Failed:  $FAILED" 
            echo "   ‚è≠Ô∏è Skipped: $SKIPPED"
            echo "   üìä Total:   $TOTAL"
            echo "   ‚è±Ô∏è Duration: ${DURATION}s"
            echo ""
            
            # Display failed tests with AI analysis
            if [ "$FAILED" -gt 0 ]; then
              echo "üîç FAILED TESTS WITH AI INSIGHTS:"
              echo "============================================================"
              
              # Extract failed tests and their AI analysis from CTRF
              cat test-results/ctrf-report.json | jq -r '
                .results.tests[] | 
                select(.status == "failed") | 
                "üö® Test: " + .name + "\n" +
                "   Suite: " + (.suite // "N/A") + "\n" +
                "   Duration: " + (.duration | tostring) + "ms\n" +
                if .ai then "   ü§ñ AI Analysis: " + (.ai | gsub("\n"; "\n      ")) + "\n" else "   ü§ñ AI Analysis: Not available\n" end +
                "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
              '
            else
              echo "üéâ All tests passed! No failures to analyze."
            fi
            
            # Display overall AI summary if available
            OVERALL_AI=$(cat test-results/ctrf-report.json | jq -r '.results.extra.ai // empty')
            if [ ! -z "$OVERALL_AI" ]; then
              echo ""
              echo "üìã OVERALL AI SUMMARY:"
              echo "============================================================"
              echo "$OVERALL_AI"
            fi
            
          else
            echo "‚ùå No test results found to display"
          fi
          
          echo ""
          echo "============================================================"
          echo "üìÅ Full AI report available in artifacts: 'comprehensive-ai-analysis'"
          echo "============================================================"

      - name: üìã Upload Comprehensive AI Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-ai-analysis
          path: |
            ai-test-report-*.html
            test-results/ctrf-report.json
            consolidated-reports/
          retention-days: 30

      - name: üìä Generate Test Summary for PR
        if: always()
        run: |
          echo "Generating test summary for GitHub PR comment..."
          if [ -f "test-results/ctrf-report.json" ]; then
            echo "TEST_SUMMARY<<EOF" >> $GITHUB_ENV
            cat test-results/ctrf-report.json | jq -r '
              .results.summary as $summary |
              .results.tests as $tests |
              "## üé≠ Playwright Test Results\n\n" +
              "| Metric | Value |\n" +
              "|--------|---------|\n" +
              "| ‚úÖ Passed | \($summary.passed) |\n" +
              "| ‚ùå Failed | \($summary.failed) |\n" +
              "| ‚è≠Ô∏è Skipped | \($summary.skipped) |\n" +
              "| üìä Total | \($summary.tests) |\n" +
              "| ‚è±Ô∏è Duration | \(($summary.stop - $summary.start) / 1000 | floor)s |\n\n" +
              "**Environment:** " + (env.TEST_ENV // "staging") + "\n\n" +
              (if $summary.failed > 0 then 
                "### ü§ñ AI Analysis Summary\n\n" +
                ([$tests[] | select(.status == "failed")] | map("- **" + .name + "**: " + (if .ai then (.ai | split("\n")[0] | .[0:100] + (if length > 100 then "..." else "" end)) else "Analysis pending" end)) | join("\n")) + "\n\n"
              else 
                "### üéâ All Tests Passed!\nNo failures to analyze.\n\n" 
              end) +
              "üìÅ **Full AI Report:** Download `comprehensive-ai-analysis` artifact for detailed insights"
            ' >> $GITHUB_ENV
            echo "EOF" >> $GITHUB_ENV
          else
            echo "TEST_SUMMARY=No test results available" >> $GITHUB_ENV
          fi

  # Report consolidation job
  consolidate-reports:
    name: üìä Consolidate Reports
    runs-on: ubuntu-latest
    needs: [test, ai-analysis]
    if: always()
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: ÔøΩ Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*-report-*"
          path: all-artifacts/
          merge-multiple: true

      - name: ÔøΩ Download AI analysis
        uses: actions/download-artifact@v4
        with:
          name: comprehensive-ai-analysis
          path: ai-analysis/

      - name: ÔøΩ Create Final Report Package
        run: |
          mkdir -p final-reports
          
          # Copy CTRF reports
          find all-artifacts -name "ctrf-report.json" -exec cp {} final-reports/ \; 2>/dev/null || true
          
          # Copy Playwright reports
          find all-artifacts -name "index.html" -path "*/playwright-report/*" -exec cp {} final-reports/playwright-report.html \; 2>/dev/null || true
          
          # Copy AI analysis
          cp ai-analysis/ai-test-report-*.html final-reports/ 2>/dev/null || true
          cp ai-analysis/test-results/ctrf-report.json final-reports/consolidated-ctrf-report.json 2>/dev/null || true
          
          # Generate a summary index
          cat > final-reports/README.md << 'EOF'
          # Test Report Summary
          
          This package contains comprehensive test results and AI analysis.
          
          ## Files Included:
          - `ai-test-report-*.html` - AI-powered analysis of test failures
          - `playwright-report.html` - Standard Playwright HTML report  
          - `ctrf-report.json` - Machine-readable test results
          - `consolidated-ctrf-report.json` - Final consolidated results
          
          ## How to Use:
          1. Open the AI test report HTML file in a browser for intelligent insights
          2. Review the Playwright report for detailed test execution logs
          3. Use the CTRF JSON for programmatic analysis
          
          Generated by: Playwright Tests with Advanced Reporting
          EOF
          
          echo "Final report package contents:"
          ls -la final-reports/

      - name: ÔøΩ Upload Final Report Package  
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: complete-test-report-package
          path: final-reports/
          retention-days: 90

      - name: üìà Comment PR with Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const testSummary = process.env.TEST_SUMMARY || "No test results available";
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: testSummary + "\n\n" +
                    "---\n" +
                    "ü§ñ **AI-Powered Analysis**: Intelligent test failure analysis included\n" +
                    "üìã **Workflow Logs**: Real-time AI insights visible in workflow output\n" +
                    "üì¶ **Download Reports**: Complete analysis available in workflow artifacts\n" +
                    "ÔøΩ **Quick Access**: Check the 'Display AI Analysis' steps for immediate insights"
            });
